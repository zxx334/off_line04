# fully_offline_finetune.py
import torch
import torch.nn as nn
from transformers import TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import Dataset
import json
import logging
import os
import numpy as np

# å®Œå…¨ç¦ç”¨ç½‘ç»œå’Œè­¦å‘Š
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleTokenizer:
    """å®Œå…¨ç¦»çº¿çš„ç®€å•åˆ†è¯å™¨"""
    def __init__(self):
        # åˆ›å»ºåŸºç¡€è¯æ±‡è¡¨
        self.vocab = {
            '[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4,
            '<|im_start|>': 5, '<|im_end|>': 6, 'user': 7, 'assistant': 8,
            'è¿™ä¸ª': 9, 'åœ°å€': 10, 'åœ¨': 11, 'å“ªä¸ª': 12, 'çœ': 13, 'å¸‚': 14,
            'åŒº': 15, 'å¿': 16, 'è¯·': 17, 'æ ‡å‡†åŒ–': 18, 'åŒ—äº¬': 19, 'ä¸Šæµ·': 20,
            'å¹¿ä¸œ': 21, 'æ·±åœ³': 22, 'æµ™æ±Ÿ': 23, 'æ­å·': 24, 'æ±Ÿè‹': 25, 'å—äº¬': 26,
            'å››å·': 27, 'æˆéƒ½': 28, 'é™•è¥¿': 29, 'è¥¿å®‰': 30, 'æ¹–åŒ—': 31, 'æ­¦æ±‰': 32,
            'æ¹–å—': 33, 'é•¿æ²™': 34, 'å±±ä¸œ': 35, 'æµå—': 36, 'æµ·æ·€': 37, 'æµ¦ä¸œ': 38,
            'å—å±±': 39, 'è¥¿æ¹–': 40, 'é¼“æ¥¼': 41, 'æ­¦ä¾¯': 42, 'é›å¡”': 43, 'æ­¦æ˜Œ': 44,
            'å²³éº“': 45, 'å†ä¸‹': 46, 'ä¸­å…³æ‘': 47, 'å¼ æ±Ÿ': 48, 'ç§‘æŠ€å›­': 49, 'å¤§è¡—': 50,
            'è·¯': 51, 'å·': 52, 'æ— æ³•': 53, 'ç¡®å®š': 54, 'ã€‚': 55, 'ï¼Œ': 56, 'ï¼š': 57
        }
        self.inverse_vocab = {v: k for k, v in self.vocab.items()}
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.cls_token_id = 2
        self.sep_token_id = 3

    def encode(self, text, max_length=128):
        """ç®€å•ç¼–ç æ–‡æœ¬"""
        tokens = []
        # ç®€å•çš„åˆ†è¯ï¼šæŒ‰å­—ç¬¦åˆ†å‰²ï¼Œä½†ä¿ç•™ä¸€äº›å¸¸è§è¯æ±‡
        words = text.replace('<|im_start|>', ' <|im_start|> ').replace('<|im_end|>', ' <|im_end|> ').split()

        for word in words:
            if word in self.vocab:
                tokens.append(self.vocab[word])
            else:
                # å¯¹æœªçŸ¥è¯æŒ‰å­—ç¬¦å¤„ç†
                for char in word:
                    if char in self.vocab:
                        tokens.append(self.vocab[char])
                    else:
                        tokens.append(self.unk_token_id)

        # æˆªæ–­æˆ–å¡«å……
        if len(tokens) > max_length:
            tokens = tokens[:max_length]
        else:
            tokens = tokens + [self.pad_token_id] * (max_length - len(tokens))

        return tokens

    def decode(self, token_ids):
        """è§£ç token IDs"""
        tokens = []
        for token_id in token_ids:
            if token_id in self.inverse_vocab:
                tokens.append(self.inverse_vocab[token_id])
            else:
                tokens.append('[UNK]')
        return ' '.join(tokens)

class SimpleModel(nn.Module):
    """ç®€å•çš„Transformeræ¨¡å‹"""
    def __init__(self, vocab_size=58, hidden_size=128, num_layers=4, num_heads=4):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size

        # è¯åµŒå…¥
        self.embedding = nn.Embedding(vocab_size, hidden_size)

        # ç®€å•çš„Transformerå±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # è¾“å‡ºå±‚
        self.output = nn.Linear(hidden_size, vocab_size)

    def forward(self, input_ids, attention_mask=None, labels=None):
        # åµŒå…¥
        x = self.embedding(input_ids)

        # Transformer
        x = self.transformer(x)

        # è¾“å‡º
        logits = self.output(x)

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.vocab_size), labels.view(-1))

        return {'loss': loss, 'logits': logits}

    def generate(self, input_ids, max_length=50, temperature=1.0):
        """ç®€å•çš„ç”Ÿæˆæ–¹æ³•"""
        with torch.no_grad():
            for _ in range(max_length - input_ids.shape[1]):
                outputs = self.forward(input_ids)
                next_token_logits = outputs['logits'][:, -1, :] / temperature

                # é‡‡æ ·
                probabilities = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probabilities, num_samples=1)

                input_ids = torch.cat([input_ids, next_token], dim=1)

        return input_ids

class FullyOfflineFinetuner:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.lora_config = None

    def setup_model(self):
        """è®¾ç½®å®Œå…¨ç¦»çº¿çš„æ¨¡å‹"""
        logger.info("è®¾ç½®å®Œå…¨ç¦»çº¿æ¨¡å‹...")

        self.tokenizer = SimpleTokenizer()

        # åˆ›å»ºç®€å•æ¨¡å‹
        self.model = SimpleModel(
            vocab_size=len(self.tokenizer.vocab),
            hidden_size=128,
            num_layers=2,
            num_heads=4
        )

        logger.info(f"æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(self.tokenizer.vocab)}")

    def setup_lora(self):
        """é…ç½®LoRA"""
        logger.info("é…ç½®LoRA...")

        # ä¸ºç®€å•æ¨¡å‹é…ç½®LoRA
        self.lora_config = LoraConfig(
            r=4,
            lora_alpha=16,
            target_modules=["embedding", "output"],  # é’ˆå¯¹ç®€å•æ¨¡å‹çš„ç›®æ ‡æ¨¡å—
            lora_dropout=0.1,
        )

        self.model = get_peft_model(self.model, self.lora_config)

        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params}/{total_params} ({trainable_params/total_params*100:.2f}%)")

    def load_training_data(self, data_path):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"åŠ è½½äº† {len(data)} æ¡è®­ç»ƒæ•°æ®")
        return data

    def preprocess_data(self, examples):
        """é¢„å¤„ç†æ•°æ®"""
        input_ids = []
        labels = []

        for instruction, output in zip(examples["instruction"], examples["output"]):
            # æ„å»ºè®­ç»ƒæ–‡æœ¬
            text = f"<|im_start|>user {instruction} <|im_end|> <|im_start|>assistant {output} <|im_end|>"

            # ç¼–ç 
            tokens = self.tokenizer.encode(text)
            input_ids.append(tokens)
            labels.append(tokens.copy())  # å¯¹äºè¯­è¨€æ¨¡å‹ï¼Œæ ‡ç­¾å°±æ˜¯è¾“å…¥

        return {
            "input_ids": input_ids,
            "labels": labels,
            "attention_mask": [[1] * len(ids) for ids in input_ids]
        }

    def train(self, data_path, output_dir):
        """æ‰§è¡Œè®­ç»ƒ"""
        logger.info("å¼€å§‹å®Œå…¨ç¦»çº¿ LoRA å¾®è°ƒ...")

        try:
            # 1. è®¾ç½®æ¨¡å‹
            self.setup_model()

            # 2. é…ç½®LoRA
            self.setup_lora()

            # 3. å‡†å¤‡æ•°æ®
            raw_data = self.load_training_data(data_path)
            dataset = Dataset.from_dict({
                "instruction": [item["instruction"] for item in raw_data],
                "output": [item["output"] for item in raw_data]
            })

            processed_dataset = dataset.map(
                self.preprocess_data,
                batched=True,
                remove_columns=dataset.column_names
            )

            logger.info(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(processed_dataset)}")

            # 4. è®­ç»ƒå‚æ•°
            training_args = TrainingArguments(
                output_dir=output_dir,
                num_train_epochs=5,
                per_device_train_batch_size=2,
                learning_rate=1e-3,
                logging_steps=1,
                save_steps=10,
                remove_unused_columns=False,
                report_to=None,
            )

            # 5. è®­ç»ƒå™¨
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=processed_dataset,
            )

            # 6. å¼€å§‹è®­ç»ƒ
            logger.info("å¯åŠ¨è®­ç»ƒè¿‡ç¨‹...")
            train_result = trainer.train()

            # 7. ä¿å­˜æ¨¡å‹
            trainer.save_model()

            # ä¿å­˜åˆ†è¯å™¨ä¿¡æ¯
            tokenizer_info = {
                "vocab": self.tokenizer.vocab,
                "inverse_vocab": self.tokenizer.inverse_vocab
            }
            with open(os.path.join(output_dir, "tokenizer.json"), "w", encoding="utf-8") as f:
                json.dump(tokenizer_info, f, ensure_ascii=False, indent=2)

            logger.info(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: {output_dir}")
            logger.info(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_result.metrics['train_loss']:.4f}")

            return True

        except Exception as e:
            logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

if __name__ == "__main__":
    finetuner = FullyOfflineFinetuner()
    success = finetuner.train(
        data_path="training_data/qwen_address_training.json",
        output_dir="models/fully_offline_lora"
    )

    if success:
        print("ğŸ‰ å®Œå…¨ç¦»çº¿ LoRA å¾®è°ƒæˆåŠŸå®Œæˆï¼")
        print("æ¨¡å‹ä¿å­˜åœ¨: models/fully_offline_lora")
    else:
        print("âŒ å®Œå…¨ç¦»çº¿ LoRA å¾®è°ƒå¤±è´¥")