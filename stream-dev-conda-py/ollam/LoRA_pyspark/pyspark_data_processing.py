# pyspark_data_processing.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import Window
import json
import re
import os
import findspark
findspark.init()

class PySparkAddressProcessor:
    def __init__(self):
        """åˆå§‹åŒ– Spark ä¼šè¯"""
        print("åˆå§‹åŒ– Spark ä¼šè¯...")
        self.spark = SparkSession.builder \
            .appName("AddressDataProcessing") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.executor.memory", "1g") \
            .config("spark.driver.memory", "1g") \
            .config("spark.python.worker.reuse", "true") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
            .config("spark.sql.parquet.enableVectorizedReader", "false") \
            .config("spark.sql.warehouse.dir", "file:///tmp/spark-warehouse") \
            .master("local[1]") \
            .getOrCreate()

        # æ­£ç¡®çš„ç¼©è¿›
        self.spark.sparkContext.setLogLevel("WARN")
        print("Spark ä¼šè¯åˆå§‹åŒ–å®Œæˆ")

    def create_sample_data(self):
        """åˆ›å»ºç¤ºä¾‹åœ°å€æ•°æ® - ç®€åŒ–ç‰ˆæœ¬é¿å…æ–‡ä»¶å†™å…¥é—®é¢˜"""
        print("åˆ›å»ºç¤ºä¾‹åœ°å€æ•°æ®...")

        sample_data = [
            ("åŒ—äº¬å¸‚æµ·æ·€åŒºä¸­å…³æ‘å¤§è¡—1å·", "åŒ—äº¬", "åŒ—äº¬å¸‚", "æµ·æ·€åŒº", "ä¸­å…³æ‘å¤§è¡—"),
            ("ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºå¼ æ±Ÿé«˜ç§‘æŠ€å›­åŒº", "ä¸Šæµ·", "ä¸Šæµ·å¸‚", "æµ¦ä¸œæ–°åŒº", "å¼ æ±Ÿé«˜ç§‘æŠ€å›­åŒº"),
            ("å¹¿ä¸œçœæ·±åœ³å¸‚å—å±±åŒºç§‘æŠ€å›­å—è·¯100å·", "å¹¿ä¸œ", "æ·±åœ³å¸‚", "å—å±±åŒº", "ç§‘æŠ€å›­å—è·¯"),
            ("æµ™æ±Ÿçœæ­å·å¸‚è¥¿æ¹–åŒºæ–‡ä¸‰è·¯100å·", "æµ™æ±Ÿ", "æ­å·å¸‚", "è¥¿æ¹–åŒº", "æ–‡ä¸‰è·¯"),
            ("æ±Ÿè‹çœå—äº¬å¸‚é¼“æ¥¼åŒºä¸­å±±è·¯1å·", "æ±Ÿè‹", "å—äº¬å¸‚", "é¼“æ¥¼åŒº", "ä¸­å±±è·¯"),
            ("å››å·çœæˆéƒ½å¸‚æ­¦ä¾¯åŒºå¤©åºœè½¯ä»¶å›­", "å››å·", "æˆéƒ½å¸‚", "æ­¦ä¾¯åŒº", "å¤©åºœè½¯ä»¶å›­"),
            ("é™•è¥¿çœè¥¿å®‰å¸‚é›å¡”åŒºé«˜æ–°è·¯", "é™•è¥¿", "è¥¿å®‰å¸‚", "é›å¡”åŒº", "é«˜æ–°è·¯"),
            ("æ¹–åŒ—çœæ­¦æ±‰å¸‚æ­¦æ˜ŒåŒºä¸œæ¹–è·¯", "æ¹–åŒ—", "æ­¦æ±‰å¸‚", "æ­¦æ˜ŒåŒº", "ä¸œæ¹–è·¯"),
            ("æ¹–å—çœé•¿æ²™å¸‚å²³éº“åŒºæ©˜å­æ´²å¤´", "æ¹–å—", "é•¿æ²™å¸‚", "å²³éº“åŒº", "æ©˜å­æ´²å¤´"),
            ("å±±ä¸œçœæµå—å¸‚å†ä¸‹åŒºæ³‰åŸå¹¿åœº", "å±±ä¸œ", "æµå—å¸‚", "å†ä¸‹åŒº", "æ³‰åŸå¹¿åœº")
        ]

        # åˆ›å»º DataFrame
        schema = ["address", "province", "city", "district", "street"]
        df = self.spark.createDataFrame(sample_data, schema)

        print("ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆ")
        return df

    def clean_address_data(self, df):
        """æ¸…æ´—åœ°å€æ•°æ®"""
        print("æ¸…æ´—åœ°å€æ•°æ®...")

        # å®šä¹‰æ¸…æ´—UDF
        def clean_address_udf(address):
            if not address:
                return ""
            # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
            cleaned = re.sub(r'[^\w\u4e00-\u9fff\s\-\.]', '', str(address))
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()
            return cleaned

        clean_udf = udf(clean_address_udf, StringType())

        # åº”ç”¨æ¸…æ´—
        df_cleaned = df.filter(col("address").isNotNull()) \
            .withColumn("cleaned_address", clean_udf(col("address"))) \
            .filter(length(col("cleaned_address")) >= 3)

        print(f"æ¸…æ´—åæ•°æ®é‡: {df_cleaned.count()} æ¡")
        return df_cleaned

    def extract_address_features(self, df):
        """æå–åœ°å€ç‰¹å¾"""
        print("æå–åœ°å€ç‰¹å¾...")

        # å®šä¹‰ç‰¹å¾æå–UDF
        def extract_features_udf(address):
            features = {
                "has_province": 0,
                "has_city": 0,
                "has_district": 0,
                "has_street": 0,
                "address_length": len(address),
                "word_count": len(address.strip())
            }

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å„çº§è¡Œæ”¿åŒºåˆ’å…³é”®è¯
            province_keywords = ['çœ', 'è‡ªæ²»åŒº']
            city_keywords = ['å¸‚', 'å·']
            district_keywords = ['åŒº', 'å¿', 'æ——']
            street_keywords = ['è¡—', 'è·¯', 'é“', 'å··', 'èƒ¡åŒ']

            for keyword in province_keywords:
                if keyword in address:
                    features["has_province"] = 1
                    break

            for keyword in city_keywords:
                if keyword in address:
                    features["has_city"] = 1
                    break

            for keyword in district_keywords:
                if keyword in address:
                    features["has_district"] = 1
                    break

            for keyword in street_keywords:
                if keyword in address:
                    features["has_street"] = 1
                    break

            return json.dumps(features)

        features_udf = udf(extract_features_udf, StringType())

        df_with_features = df.withColumn("features", features_udf(col("cleaned_address"))) \
            .withColumn("features_json", from_json(col("features"),
                                                   StructType([
                                                       StructField("has_province", IntegerType()),
                                                       StructField("has_city", IntegerType()),
                                                       StructField("has_district", IntegerType()),
                                                       StructField("has_street", IntegerType()),
                                                       StructField("address_length", IntegerType()),
                                                       StructField("word_count", IntegerType())
                                                   ])))

        # å±•å¼€ç‰¹å¾åˆ—
        for field in ["has_province", "has_city", "has_district", "has_street", "address_length", "word_count"]:
            df_with_features = df_with_features.withColumn(field, col(f"features_json.{field}"))

        return df_with_features.drop("features", "features_json")

    def generate_training_pairs(self, df):
        """ç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹"""
        print("ç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹...")

        # å®šä¹‰è®­ç»ƒæ•°æ®ç”ŸæˆUDF
        def create_training_pairs_udf(address, province, city, district, street):
            pairs = []

            base_address = address

            # ç”Ÿæˆçœç›¸å…³é—®ç­”
            if province and province != "æœªçŸ¥":
                pairs.append({
                    "instruction": f"è¿™ä¸ªåœ°å€åœ¨å“ªä¸ªçœï¼Ÿ{base_address}",
                    "output": f"è¿™ä¸ªåœ°å€åœ¨{province}çœ"
                })

            # ç”Ÿæˆå¸‚ç›¸å…³é—®ç­”
            if city and city != "æœªçŸ¥":
                pairs.append({
                    "instruction": f"è¿™ä¸ªåœ°å€åœ¨å“ªä¸ªå¸‚ï¼Ÿ{base_address}",
                    "output": f"è¿™ä¸ªåœ°å€åœ¨{city}"
                })

            # ç”ŸæˆåŒºå¿ç›¸å…³é—®ç­”
            if district and district != "æœªçŸ¥":
                pairs.append({
                    "instruction": f"è¿™ä¸ªåœ°å€åœ¨å“ªä¸ªåŒº/å¿ï¼Ÿ{base_address}",
                    "output": f"è¿™ä¸ªåœ°å€åœ¨{district}"
                })

            # ç”Ÿæˆæ ‡å‡†åŒ–é—®ç­”
            standardized = f"{province if province and province != 'æœªçŸ¥' else ''}{city if city and city != 'æœªçŸ¥' else ''}{district if district and district != 'æœªçŸ¥' else ''}{street if street and street != 'æœªçŸ¥' else ''}".strip()
            if standardized:
                pairs.append({
                    "instruction": f"è¯·æ ‡å‡†åŒ–è¿™ä¸ªåœ°å€ï¼š{base_address}",
                    "output": f"æ ‡å‡†åŒ–åœ°å€ï¼š{standardized}"
                })

            return json.dumps(pairs)

        training_udf = udf(create_training_pairs_udf, StringType())

        df_with_pairs = df.withColumn("training_pairs", training_udf(
            col("cleaned_address"), col("province"), col("city"), col("district"), col("street")
        ))

        # å±•å¼€è®­ç»ƒå¯¹
        df_exploded = df_with_pairs.withColumn("pair", explode(from_json(col("training_pairs"),
                                                                         ArrayType(StructType([
                                                                             StructField("instruction", StringType()),
                                                                             StructField("output", StringType())
                                                                         ]))
                                                                         )))

        final_df = df_exploded.select(
            col("pair.instruction").alias("instruction"),
            col("pair.output").alias("output"),
            col("cleaned_address").alias("original_address"),
            col("province"),
            col("city"),
            col("district"),
            col("street")
        ).distinct()

        print(f"ç”Ÿæˆçš„è®­ç»ƒæ•°æ®é‡: {final_df.count()} æ¡")
        return final_df

    def save_training_data(self, df, output_path):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        print(f"ä¿å­˜è®­ç»ƒæ•°æ®åˆ°: {output_path}")

        # è½¬æ¢ä¸ºPandas DataFrameä¿å­˜ï¼ˆé€‚åˆä¸­å°æ•°æ®é›†ï¼‰
        pandas_df = df.select("instruction", "output", "original_address").toPandas()

        # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        training_data = []
        for _, row in pandas_df.iterrows():
            training_data.append({
                "instruction": row['instruction'],
                "input": "",
                "output": row['output'],
                "original_address": row['original_address']
            })

        # ä¿å­˜ä¸ºJSON
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)

        print(f"è®­ç»ƒæ•°æ®ä¿å­˜å®Œæˆ: {len(training_data)} æ¡")
        return training_data

    def process_pipeline(self, output_path="training_data/pyspark_address_training.json"):
        """å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿"""
        print("=== å¼€å§‹ PySpark åœ°å€æ•°æ®å¤„ç†æµæ°´çº¿ ===")

        try:
            # 1. åˆ›å»ºæ•°æ®
            print("ä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®...")
            df = self.create_sample_data()

            # 2. æ•°æ®æ¸…æ´—
            df_cleaned = self.clean_address_data(df)

            # 3. ç‰¹å¾æå–
            df_features = self.extract_address_features(df_cleaned)

            # 4. ç”Ÿæˆè®­ç»ƒæ•°æ®
            df_training = self.generate_training_pairs(df_features)

            # 5. ä¿å­˜è®­ç»ƒæ•°æ®
            training_data = self.save_training_data(df_training, output_path)

            # 6. æ˜¾ç¤ºæ ·æœ¬
            print("\n=== è®­ç»ƒæ•°æ®æ ·æœ¬ ===")
            df_training.select("instruction", "output").show(10, truncate=False)

            print("=== PySpark æ•°æ®å¤„ç†å®Œæˆ ===")
            return training_data

        except Exception as e:
            print(f"PySparkå¤„ç†å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•å¤„ç†
            return self.fallback_processing(output_path)

    def fallback_processing(self, output_path):
        """PySparkå¤±è´¥æ—¶çš„å›é€€å¤„ç†"""
        print("ä½¿ç”¨å›é€€æ•°æ®å¤„ç†...")
        sample_data = [
            {"address": "åŒ—äº¬å¸‚æµ·æ·€åŒºä¸­å…³æ‘å¤§è¡—1å·", "province": "åŒ—äº¬", "city": "åŒ—äº¬å¸‚", "district": "æµ·æ·€åŒº", "street": "ä¸­å…³æ‘å¤§è¡—"},
            {"address": "ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºå¼ æ±Ÿé«˜ç§‘æŠ€å›­åŒº", "province": "ä¸Šæµ·", "city": "ä¸Šæµ·å¸‚", "district": "æµ¦ä¸œæ–°åŒº", "street": "å¼ æ±Ÿé«˜ç§‘æŠ€å›­åŒº"},
        ]

        training_data = []
        for item in sample_data:
            training_data.extend([
                {
                    "instruction": f"è¿™ä¸ªåœ°å€åœ¨å“ªä¸ªçœï¼Ÿ{item['address']}",
                    "input": "",
                    "output": f"è¿™ä¸ªåœ°å€åœ¨{item['province']}çœ",
                    "original_address": item['address']
                },
                {
                    "instruction": f"è¿™ä¸ªåœ°å€åœ¨å“ªä¸ªå¸‚ï¼Ÿ{item['address']}",
                    "input": "",
                    "output": f"è¿™ä¸ªåœ°å€åœ¨{item['city']}",
                    "original_address": item['address']
                }
            ])

        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)

        print(f"å›é€€å¤„ç†å®Œæˆ: {len(training_data)} æ¡")
        return training_data

    def stop_spark(self):
        """åœæ­¢ Spark ä¼šè¯"""
        self.spark.stop()
        print("Spark ä¼šè¯å·²åœæ­¢")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    processor = PySparkAddressProcessor()

    try:
        # è¿è¡Œå®Œæ•´æµæ°´çº¿
        training_data = processor.process_pipeline(
            output_path="training_data/pyspark_address_training.json"
        )

        print(f"\nğŸ‰ PySpark æ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"ç”Ÿæˆè®­ç»ƒæ ·æœ¬: {len(training_data)} æ¡")
        print(f"è¾“å‡ºæ–‡ä»¶: training_data/pyspark_address_training.json")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        processor.stop_spark()