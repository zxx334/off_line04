import requests
import json
import time
import subprocess
import sys
import psutil

class QwenGPUDeployment:
    def __init__(self):
        self.base_url = "http://localhost:11434/api"
        self.model = "qwen3:0.6b"

    def check_gpu_availability(self):
        """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯"""
        gpu_info = {
            "available": False,
            "gpu_memory_total": 0,
            "gpu_memory_used": 0,
            "gpu_memory_free": 0,
            "gpu_name": "Unknown"
        }

        try:
            # æ£€æŸ¥Ollamaæ˜¯å¦æ”¯æŒGPU
            response = requests.get(f"{self.base_url}/version")
            version_info = response.json()
            print(f"Ollamaç‰ˆæœ¬: {version_info}")

            # æ£€æŸ¥NVIDIA GPU
            try:
                result = subprocess.run(['nvidia-smi',
                                         '--query-gpu=name,memory.total,memory.used,memory.free',
                                         '--format=csv,noheader,nounits'],
                                        capture_output=True, text=True, encoding='utf-8')

                if result.returncode == 0:
                    gpu_data = result.stdout.strip().split(', ')
                    if len(gpu_data) >= 4:
                        gpu_info.update({
                            "available": True,
                            "gpu_name": gpu_data[0],
                            "gpu_memory_total": int(gpu_data[1]),
                            "gpu_memory_used": int(gpu_data[2]),
                            "gpu_memory_free": int(gpu_data[3])
                        })

                    print("âœ… NVIDIA GPU å¯ç”¨")
                    print(f"GPUå‹å·: {gpu_info['gpu_name']}")
                    print(f"æ˜¾å­˜æ€»é‡: {gpu_info['gpu_memory_total']} MB")
                    print(f"å·²ç”¨æ˜¾å­˜: {gpu_info['gpu_memory_used']} MB")
                    print(f"å‰©ä½™æ˜¾å­˜: {gpu_info['gpu_memory_free']} MB")

            except Exception as e:
                print(f"âŒ GPUæ£€æµ‹å¤±è´¥: {e}")
                gpu_info["available"] = False

        except Exception as e:
            print(f"Ollamaè¿æ¥å¤±è´¥: {e}")

        return gpu_info

    def get_system_memory_usage(self):
        """è·å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory = psutil.virtual_memory()
        return {
            "total_memory": memory.total // (1024 * 1024),  # MB
            "used_memory": memory.used // (1024 * 1024),    # MB
            "available_memory": memory.available // (1024 * 1024),  # MB
            "memory_percent": memory.percent
        }

    def get_ollama_memory_usage(self):
        """è·å–Ollamaè¿›ç¨‹å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            for proc in psutil.process_iter(['pid', 'name', 'memory_info']):
                if 'ollama' in proc.info['name'].lower():
                    memory_mb = proc.info['memory_info'].rss // (1024 * 1024)
                    return {
                        "ollama_memory_mb": memory_mb,
                        "ollama_pid": proc.info['pid']
                    }
        except Exception as e:
            print(f"è·å–Ollamaå†…å­˜ä½¿ç”¨å¤±è´¥: {e}")

        return {"ollama_memory_mb": 0, "ollama_pid": None}

    def monitor_gpu_memory_real_time(self):
        """å®æ—¶ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            result = subprocess.run(['nvidia-smi',
                                     '--query-gpu=memory.used,memory.total,utilization.gpu',
                                     '--format=csv,noheader,nounits'],
                                    capture_output=True, text=True, encoding='utf-8')

            if result.returncode == 0:
                gpu_data = result.stdout.strip().split(', ')
                if len(gpu_data) >= 3:
                    used_memory = int(gpu_data[0])
                    total_memory = int(gpu_data[1])
                    gpu_utilization = int(gpu_data[2])

                    return {
                        "gpu_used_mb": used_memory,
                        "gpu_total_mb": total_memory,
                        "gpu_utilization_percent": gpu_utilization,
                        "gpu_usage_percent": (used_memory / total_memory) * 100
                    }
        except Exception as e:
            print(f"GPUå®æ—¶ç›‘æ§å¤±è´¥: {e}")

        return None

    def generate_with_gpu(self, prompt):
        """GPUä¼˜åŒ–è°ƒç”¨ï¼ŒåŒ…å«å†…å­˜ç›‘æ§"""
        # è°ƒç”¨å‰ç›‘æ§
        pre_gpu_info = self.monitor_gpu_memory_real_time()
        pre_system_memory = self.get_system_memory_usage()
        pre_ollama_memory = self.get_ollama_memory_usage()

        print(f"\nğŸ“Š è°ƒç”¨å‰çŠ¶æ€:")
        if pre_gpu_info:
            print(f"GPUä½¿ç”¨: {pre_gpu_info['gpu_used_mb']}/{pre_gpu_info['gpu_total_mb']} MB ({pre_gpu_info['gpu_usage_percent']:.1f}%)")
        print(f"ç³»ç»Ÿå†…å­˜: {pre_system_memory['used_memory']}/{pre_system_memory['total_memory']} MB ({pre_system_memory['memory_percent']:.1f}%)")
        print(f"Ollamaå†…å­˜: {pre_ollama_memory['ollama_memory_mb']} MB")

        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_ctx": 4096,
                "temperature": 0.7,
                "num_gpu": 1,     # ä½¿ç”¨GPUå±‚
                "num_thread": 8
            }
        }

        try:
            response = requests.post(f"{self.base_url}/generate",
                                     json=payload, timeout=30)
            result = response.json()

            # è°ƒç”¨åç›‘æ§
            post_gpu_info = self.monitor_gpu_memory_real_time()
            post_system_memory = self.get_system_memory_usage()
            post_ollama_memory = self.get_ollama_memory_usage()

            print(f"\nğŸ“Š è°ƒç”¨åçŠ¶æ€:")
            if post_gpu_info:
                print(f"GPUä½¿ç”¨: {post_gpu_info['gpu_used_mb']}/{post_gpu_info['gpu_total_mb']} MB ({post_gpu_info['gpu_usage_percent']:.1f}%)")
                if pre_gpu_info:
                    gpu_increase = post_gpu_info['gpu_used_mb'] - pre_gpu_info['gpu_used_mb']
                    print(f"GPUå†…å­˜å¢åŠ : {gpu_increase} MB")

            ollama_increase = post_ollama_memory['ollama_memory_mb'] - pre_ollama_memory['ollama_memory_mb']
            print(f"Ollamaå†…å­˜å¢åŠ : {ollama_increase} MB")

            return result.get('response', 'ç”Ÿæˆå¤±è´¥')

        except Exception as e:
            return f"GPUéƒ¨ç½²è°ƒç”¨å¤±è´¥: {str(e)}"

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    gpu_deploy = QwenGPUDeployment()

    print("=== Qwen3-0.6B GPU éƒ¨ç½²æµ‹è¯• ===")
    gpu_info = gpu_deploy.check_gpu_availability()

    if not gpu_info["available"]:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†å›é€€åˆ°CPUæ¨¡å¼")

    while True:
        user_input = input("\nè¯·è¾“å…¥é—®é¢˜ (è¾“å…¥ 'quit' é€€å‡º): ")
        if user_input.lower() == 'quit':
            break

        start_time = time.time()
        response = gpu_deploy.generate_with_gpu(user_input)
        end_time = time.time()

        print(f"\nğŸ¤– å›ç­”: {response}")
        print(f"â±ï¸  å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"ğŸš€ éƒ¨ç½²æ¨¡å¼: {'GPU' if gpu_info['available'] else 'CPU'}")